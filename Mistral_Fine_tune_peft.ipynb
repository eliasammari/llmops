{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy2ZMTlT8PET"
      },
      "source": [
        "# Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "E4D6xVdF8PEU"
      },
      "source": [
        "<p></p>\n",
        "<a href=\"https://raw.githubusercontent.com/mlflow/mlflow/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb\" class=\"notebook-download-btn\">Download this Notebook</a><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp1vXoxs8PEV"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Many powerful open-source LLMs have emerged and are easily accessible. However, they are not designed to be deployed to your production environment out-of-the-box; instead, you have to **fine-tune** them for your specific tasks, such as a chatbot, content generation, etc. One challenge, though, is that training LLMs is usually very expensive. Even if your dataset for fine-tuning is small, the backpropagation step needs to compute gradients for billions of parameters. For example, fully fine-tuning the Llama7B model requires 112GB of VRAM, i.e. at least two 80GB A100 GPUs. Fortunately, there are many research efforts on how to reduce the cost of LLM fine-tuning.\n",
        "\n",
        "In this tutorial, we will demonstrate how to build a powerful **text-to-pythonL** generator by fine-tuning the Mistral 7B model with **a single 24GB VRAM GPU**.\n",
        "\n",
        "### What You Will Learn\n",
        "1. Hands-on learning of the typical LLM fine-tuning process.\n",
        "2. Understand how to use **QLoRA** and **PEFT** to overcome the GPU memory limitation for fine-tuning.\n",
        "3. Manage the model training cycle using **MLflow** to log the model artifacts, hyperparameters, metrics, and prompts.\n",
        "4. How to save prompt template and inference parameters (e.g. max_token_length) in MLflow to simplify prediction interface.\n",
        "\n",
        "### Key Actors\n",
        "In this tutorial, you will learn about the techniques and methods behind efficient LLM fine-tuning by actually running the code. There are more detailed explanations for each cell below, but let's start with a brief preview of a few main important libraries/methods used in this tutorial.\n",
        "\n",
        "* [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) model is a pretrained text-generation model with 7 billion parameters, developed by [mistral.ai](https://mistral.ai/). The model employs various optimization techniques such as Group-Query Attention, Sliding-Window Attention, Byte-fallback BPE tokenizer, and outperforms the Llama 2 13B on benchmarks with fewer parameters.\n",
        "* [QLoRA](https://github.com/artidoro/qlora) is a novel method that allows us to fine-tune large foundational models with limited GPU resources. It reduces the number of trainable parameters by learning pairs of rank-decomposition matrices and also applies 4-bit quantization to the frozen pretrained model to further reduce the memory footprint.\n",
        "* [PEFT](https://huggingface.co/docs/peft/en/index) is a library developed by HuggingFaceðŸ¤—, that enables developers to easily integrate various optimization methods with pretrained models available on the HuggingFace Hub. With PEFT, you can apply QLoRA to the pretrained model with a few lines of configurations and run fine-tuning just like the normal Transformers model training.\n",
        "* [MLflow](https://mlflow.org/) manages an exploding number of configurations, assets, and metrics during the LLM training on your behalf. MLflow is natively integrated with Transformers and PEFT, and plays a crucial role in organizing the fine-tuning cycle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUU9lChm8PEX"
      },
      "source": [
        "## 1. Environment Set up\n",
        "\n",
        "### Hardware Requirement\n",
        "Please ensure your GPU has at least 20GB of VRAM available. This notebook has been tested on a single NVIDIA A10G GPU with 24GB of VRAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1MZgigL8PEX",
        "outputId": "ac388d48-50ca-45db-f30e-e2343239fc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 20 12:34:46 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              50W / 400W |  26267MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0pmPdYl8PEY"
      },
      "source": [
        "### Install Python Libraries\n",
        "\n",
        "This tutorial utilizes the following Python libraries:\n",
        "\n",
        "* [mlflow](https://pypi.org/project/mlflow/) - for tracking parameters, metrics, and saving trained models. Version **2.11.0 or later** is required to log PEFT models with MLflow.\n",
        "* [transformers](https://pypi.org/project/transformers/) - for defining the model, tokenizer, and trainer.\n",
        "* [peft](https://pypi.org/project/peft/) - for creating a LoRA adapter on top of the Transformer model.\n",
        "* [bitsandbytes](https://pypi.org/project/bitsandbytes/) - for loading the base model with 4-bit quantization for QLoRA.\n",
        "* [accelerate](https://pypi.org/project/accelerate/) - a dependency required by bitsandbytes.\n",
        "* [datasets](https://pypi.org/project/datasets/) - for loading the training dataset from the HuggingFace hub.\n",
        "\n",
        "**Note**: Restarting the Python kernel may be necessary after installing these dependencies.\n",
        "\n",
        "The notebook has been tested with `mlflow==2.11.0`, `transformers==4.35.2`, `peft==0.8.2`, `bitsandbytes==0.42.0`, `accelerate==0.27.2`, and `datasets==2.17.1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SC5Iekr8PEY"
      },
      "outputs": [],
      "source": [
        "%pip install transformers peft accelerate bitsandbytes datasets -q -U\n",
        "!pip install --upgrade comet-ml>=3.43.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFJijCX28PEZ"
      },
      "source": [
        "## 2. Dataset Preparation\n",
        "\n",
        "### Load Dataset from HuggingFace Hub\n",
        "\n",
        "We will use the `flytech/python-codes-25k` dataset from the [Hugging Face Hub](https://huggingface.co/datasets/flytech/python-codes-25k) for this tutorial. This dataset comprises 78.6k pairs of natural language queries and their corresponding SQL statements, making it ideal for training a text-to-SQL model. The dataset includes three columns:\n",
        "\n",
        "* `question`: A natural language question posed regarding the data.\n",
        "* `context`: Additional information about the data, such as the schema for the table being queried.\n",
        "* `answer`: The python code that represents the expected output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om9gR7yI8PEZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "dataset_name = \"flytech/python-codes-25k\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset = dataset.select(range(3000))\n",
        "\n",
        "def display_table(dataset_or_sample):\n",
        "    # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely\n",
        "    pd.set_option(\"display.max_colwidth\", None)\n",
        "    pd.set_option(\"display.width\", None)\n",
        "    pd.set_option(\"display.max_rows\", None)\n",
        "\n",
        "    if isinstance(dataset_or_sample, dict):\n",
        "        df = pd.DataFrame(dataset_or_sample, index=[0])\n",
        "    else:\n",
        "        df = pd.DataFrame(dataset_or_sample)\n",
        "\n",
        "    html = df.to_html().replace(\"\\\\n\", \"<br>\")\n",
        "    styled_html = f\"\"\"<style> .dataframe th, .dataframe tbody td {{ text-align: left; padding-right: 30px; }} </style> {html}\"\"\"\n",
        "    display(HTML(styled_html))\n",
        "\n",
        "\n",
        "#display_table(dataset.select(range(3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-3J-JX8PEa"
      },
      "source": [
        "### Split Train and Test Dataset\n",
        "The `flytech/python-codes-25k` dataset consists of a single split, \"train\". We will separate 20% of this as test samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq3Uw1o78PEa",
        "outputId": "279f6088-0e47-45e3-ff57-481dac644541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset contains 2400 text-to-python pairs\n",
            "Test dataset contains 600 text-to-python pairs\n"
          ]
        }
      ],
      "source": [
        "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(f\"Training dataset contains {len(train_dataset)} text-to-python pairs\")\n",
        "print(f\"Test dataset contains {len(test_dataset)} text-to-python pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNt0NjER8PEb"
      },
      "source": [
        "### Define Prompt Template\n",
        "\n",
        "The Mistral 7B model is a text comprehension model, so we have to construct a text prompt that incorporates the user's question, context, and our system instructions. The new `prompt` column in the dataset will contain the text prompt to be fed into the model during training. It is important to note that we also include the expected response within the prompt, allowing the model to be trained in a self-supervised manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaDi3zst8PEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "a3fa1ac1d88c49b08e70a3ad8dbfb5ed",
            "212b04d67a594e57a1f306be844e3609",
            "e6ef19b1491e430cac59c60a6950291a",
            "9fa9e8d898234985944bfafa1238649a",
            "052282dac55a49ada5e9d18800832e53",
            "3cf90cfeba154640b067776331e5f140",
            "8c8fb336b3ab4617af6d3f495ff298db",
            "4fc6ce2a3df3442eaf80bc9e8d8308af",
            "485245d2597346c3a1204c57e631b8a7",
            "6a62d660dc674acca787a01b17df23a2",
            "fb5908ebae2b4627869309ed04570dc3",
            "a5a7eb5f19dc4d1683a81ab57d430072",
            "7d795b0768d84498871efd4f00d34119",
            "7f9e3c7189814ee89efdd1e66fa3db45",
            "925f242f341f42d78a45fb360697d9fd",
            "9d79ef4e61c6497e82dc2ce0b6857317",
            "5ff5118a5adb49f681254652b125e03d",
            "0553bfc406d74b8aae4c1b32e0947f2c",
            "0005993190ff4082b8268bd15eb82e29",
            "fa21bf3805b1409ea449dd89cc716f94",
            "5d9f162336f34cc5b0a931d27f60ba74",
            "32df9fe475f64df4a6e4c03ef40e7009"
          ]
        },
        "outputId": "c51c844f-fab4-43fe-ccc4-c54b16a57c78"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3fa1ac1d88c49b08e70a3ad8dbfb5ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5a7eb5f19dc4d1683a81ab57d430072"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "PROMPT_TEMPLATE = \"\"\"You are a powerful text-to-python model. Given the data shape and natural language question, your job is to write python code that answers the question.\n",
        "\n",
        "### data:\n",
        "{context}\n",
        "\n",
        "### Question:\n",
        "{question}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "\n",
        "\n",
        "def apply_prompt_template(row):\n",
        "    prompt = PROMPT_TEMPLATE.format(\n",
        "        question=row[\"instruction\"],\n",
        "        context=row[\"input\"],\n",
        "        output=row[\"output\"],\n",
        "    )\n",
        "    return {\"prompt\": prompt}\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(apply_prompt_template)\n",
        "test_dataset = test_dataset.map(apply_prompt_template)\n",
        "#display_table(train_dataset.select(range(2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tomLyfS8PEb"
      },
      "source": [
        "### Padding the Training Dataset\n",
        "\n",
        "As a final step of dataset preparation, we need to apply **padding** to the training dataset. Padding ensures that all input sequences in a batch are of the same length.\n",
        "\n",
        "A crucial point to note is the need to *add padding to the left*. This approach is adopted because the model generates tokens autoregressively, meaning it continues from the last token. Adding padding to the right would cause the model to generate new tokens from these padding tokens, resulting in the output sequence including padding tokens in the middle.\n",
        "\n",
        "\n",
        "* Padding to right\n",
        "\n",
        "```\n",
        "Today |  is  |   a    |  cold  |  <pad>  ==generate=>  \"Today is a cold <pad> day\"\n",
        " How  |  to  | become |  <pad> |  <pad>  ==generate=>  \"How to become a <pad> <pad> great engineer\".\n",
        "```\n",
        "\n",
        "* Padding to left:\n",
        "\n",
        "```\n",
        "<pad> |  Today  |  is  |  a   |  cold     ==generate=>  \"<pad> Today is a cold day\"\n",
        "<pad> |  <pad>  |  How |  to  |  become   ==generate=>  \"<pad> <pad> How to become a great engineer\".\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e8dd964c9b4e492e85bdff50d7d0273e",
            "d9e4e467a9714caba0362e8d67c93296",
            "cc86c765403f431398067565fbcb93bd",
            "e9a8d66b30a7410bb032a3e6acf792e3",
            "1c4f2983271a4ef4982f74c9e9d5363c",
            "b808e536a3d44b4c900cb991c6955b37",
            "ca0fe1d52f1342e7a396d7daad363844",
            "fbbd78869e274b689341cc583d619c6a",
            "9f15bd98459a44c9b5aa5c74acf52fc7",
            "db1e6bff06374de18be8107fd8baa0d2",
            "2f7e06fb9fbc4601b58bf44c689547e8",
            "b0c083b87f0345f5b7cec48de96f5035",
            "853c495485204468bbc503309a19cbea",
            "75f00abd4c6047a49525f5ad6657d000",
            "fa2985ad6b9b40b09a795e849556f9eb",
            "b96c8e65c4ad49e891f1a55df774d91c",
            "ac72c70bf01c41a98ddbbd7c235a1fac",
            "42a400750a8d4e74ba05af12946e5d9f",
            "ec3de817949b4f9b99c7b2130579d835",
            "7fb73745aa6c4f7dbc5a30a297f8de59",
            "c4018070067345fea6409001c36fe650",
            "6a540ee4488a47c999fb2cda73910d52"
          ]
        },
        "id": "RIy_KG3w8PEb",
        "outputId": "cbe26b4f-43fa-4868-fa9f-a16d1bb19e3f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8dd964c9b4e492e85bdff50d7d0273e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0c083b87f0345f5b7cec48de96f5035"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# You can use a different max length if your custom dataset has shorter/longer input sequences.\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=MAX_LENGTH,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "def tokenize_and_pad_to_fixed_length(sample):\n",
        "    result = tokenizer(\n",
        "        sample[\"prompt\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_and_pad_to_fixed_length)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_and_pad_to_fixed_length)\n",
        "assert all(len(x[\"input_ids\"]) == MAX_LENGTH for x in tokenized_train_dataset)\n",
        "\n",
        "#display_table(tokenized_train_dataset.select(range(1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-pMrLfq8PEc"
      },
      "source": [
        "## 3. Load the Base Model (with 4-bit quantization)\n",
        "\n",
        "Next, we'll load the Mistral 7B model, which will serve as our base model for fine-tuning. This model can be loaded from the HuggingFace Hub repository [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) using the Transformers' `from_pretrained()` API. However, here we are also providing a `quantization_config` parameter.\n",
        "\n",
        "This parameter embodies the key technique of [QLoRA](https://github.com/artidoro/qlora) that significantly reduces memory usage during fine-tuning. The following paragraph details the method and the implications of this configuration. However, feel free to skip if it appears complex. After all, we rarely need to modify the `quantization_config` values ourselves :)\n",
        "\n",
        "**How It Works**\n",
        "\n",
        "In short, QLoRA is a combination of **Q**uantization and **LoRA**. To grasp its functionality, it's simpler to begin with LoRA. [LoRA (Low Rank Adaptation)](https://github.com/microsoft/LoRA) is a preceding method for resource-efficient fine-tuning, by reducing the number of trainable parameters through matrix decomposition. Let `W'` represent the final weight matrix from fine-tuning. In LoRA, `W'` is approximated by the sum of the original weight and its update, i.e., `W + Î”W`, then decomposing the delta part into two low-dimensional matrices, i.e., `Î”W â‰ˆ AB`. Suppose `W` is `m`x`m`, and we select a smaller `r` for the rank of `A` and `B`, where `A` is `m`x`r` and `B` is `r`x`m`. Now, the original trainable parameters, which are quadratic in size of `W` (i.e., `m^2`), after decomposition, become `2mr`. Empirically, we can choose a much smaller number for `r`, e.g., 32, 64, compared to the full weight matrix size, therefore this significantly reduces the number of parameters to train.\n",
        "\n",
        "[QLoRA](https://github.com/artidoro/qlora) extends LoRA, employing the same strategy for matrix decomposition. However, it further reduces memory usage by applying 4-bit quantization to the frozen pretrained model `W`. According to their research, the largest memory usage during LoRA fine-tuning is the backpropagation through the frozen parameters `W` to compute gradients for the adaptors `A` and `B`. Thus, quantizing `W` to 4-bit significantly reduces the overall memory consumption. This is achieved with the `load_in_4bit=True` setting shown below.\n",
        "\n",
        "Moreover, QLoRA introduces additional techniques to optimize resource usage without significantly impacting model performance. For more technical details, please refer to [the paper](https://arxiv.org/pdf/2305.14314.pdf), but we implement them by setting the following quantization configurations in bitsandbytes:\n",
        "* The 4-bit NormalFloat type is specified by `bnb_4bit_quant_type=\"nf4\"`.\n",
        "* Double quantization is activated by `bnb_4bit_use_double_quant=True`.\n",
        "* QLoRA re-quantizes the 4-bit weights back to a higher precision when computing the gradients for `A` and `B`, to prevent performance degradation. This datatype is specified by `bnb_4bit_compute_dtype=torch.bfloat16`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "01acf93846664a809b3c80514cb9e0df",
            "a60647a95d8b4cc695c1211ae2639337",
            "fd78b4af616e45f4b98b98ad6ba12cb1",
            "c43affa33aae471fa536580e7af29e3a",
            "ac34ca7b530a4865b11e5ad45861c114",
            "d39d9f016a00479992f256b3f71683a2",
            "afa592ab430240ddb725f59dc66e8fea",
            "ec1d837c1c4e4cff8183bdf2f6195083",
            "da1eb0a8ee104e17807ec46c0eea6e3e",
            "0f6a44f39ac644f6b860047b0893ed5f",
            "cd74d1099ad74691bb4f0b080cf43a4b"
          ]
        },
        "id": "nSO9fRVm8PEc",
        "outputId": "5655fe4b-adaa-4a72-e857-2348b502b5f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01acf93846664a809b3c80514cb9e0df"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    # Load the model with 4-bit quantization\n",
        "    load_in_4bit=True,\n",
        "    # Use double quantization\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    # Use 4-bit Normal Float for storing the base model weights in GPU memory\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=quantization_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GvofYEc8PEc"
      },
      "source": [
        "### How Does the Base Model Perform?\n",
        "First, let's assess the performance of the vanilla Mistral model on the SQL generation task before any fine-tuning. As expected, the model does not produce correct SQL queries; instead, it generates random answers in natural language. This outcome indicates the necessity of fine-tuning the model for our specific task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "i7A7uT0w8PEc",
        "outputId": "f4315a4f-1faa-45a5-c590-40eb41fdb57c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> .dataframe th, .dataframe tbody td { text-align: left; padding-right: 30px; } </style> <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>generated_query</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You are a powerful text-to-python model. Given the data shape and natural language question, your job is to write python code that answers the question.<br><br>### data:<br>Finding the best time to post...<br><br>### Question:<br>Help me find the best time to post on social media based on my followers!<br><br>### Response:<br></td>\n",
              "      <td>```python<br>import pandas as pd<br>from datetime import date<br><br># Load the data into a DataFrame<br>df = pd.read_csv('data/followers.csv')<br><br># Filter the data by country<br>filtered_df = df[df['country'] == 'US']<br><br># Calculate the number of followers for each hour of the day<br>hourly_counts = filtered_df.groupby(pd.Grouper(key='time', freq='H'))['follower_count'].sum()<br><br># Plot the hourly counts<br>hourly_counts.plot(kind='bar')<br>plt.title('Number of Followers per Hour')<br>plt.xlabel('Time (hours)')<br>plt.ylabel('Followers')<br>plt.show()<br><br># Find the peak hours<br>peak_hours = hourly_counts.idxmax()<br>print(\"The peak hours are:\", peak_hours)<br><br># Convert the peak hours to dates<br>peak_dates = [date(2019, 5, int(hour)) for hour in peak_hours]<br></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "pipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n",
        "\n",
        "sample = test_dataset[100]\n",
        "prompt = PROMPT_TEMPLATE.format(\n",
        "    context=sample[\"input\"], question=sample[\"instruction\"], output=\"\"\n",
        ")  # Leave the answer part blank\n",
        "import time\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    response = pipeline(prompt, max_new_tokens=256, repetition_penalty=1.15, return_full_text=False)\n",
        "duration = time.time() - start_time\n",
        "display_table({\"prompt\": prompt, \"generated_query\": response[0][\"generated_text\"]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "I4RO796QnbSj",
        "outputId": "50a2a9ee-6a22-489a-e285-e05ce517cb45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a powerful text-to-python model. Given the data shape and natural language question, your job is to write python code that answers the question.\\n\\n### data:\\n{context}\\n\\n### Question:\\n{question}\\n\\n### Response:\\n{output}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r0p8lOM8PEd"
      },
      "source": [
        "## 4. Define a PEFT Model\n",
        "\n",
        "As discussed earlier, QLoRA stands for **Quantization** + **LoRA**. Having applied the quantization part, we now proceed with the LoRA aspect. Although the mathematics behind LoRA is intricate, [PEFT](https://huggingface.co/docs/peft/en/index) helps us by simplifying the process of adapting LoRA to the pretrained Transformer model.\n",
        "\n",
        "In the next cell, we create a [LoraConfig](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py) with various settings for LoRA. Contrary to the earlier `quantization_config`, these hyperparameters might need optimization to achieve the best model performance for your specific task. **MLflow** facilitates this process by tracking these hyperparameters, the associated model, and its outcomes.\n",
        "\n",
        "At the end of the cell, we display the number of trainable parameters during fine-tuning, and their percentage relative to the total model parameters. Here, we are training only 1.16% of the total 7 billion parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czT_eR3t8PEd",
        "outputId": "ae17cde5-d650-465d-8c47-7d94f354250a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 85,041,152 || all params: 7,326,773,248 || trainable%: 1.1607\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Enabling gradient checkpointing, to make the training further efficient\n",
        "model.gradient_checkpointing_enable()\n",
        "# Set up the model for quantization-aware training e.g. casting layers, parameter freezing, etc.\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # This is the rank of the decomposed matrices A and B to be learned during fine-tuning. A smaller number will save more GPU memory but might result in worse performance.\n",
        "    r=32,\n",
        "    # This is the coefficient for the learned Î”W factor, so the larger number will typically result in a larger behavior change after fine-tuning.\n",
        "    lora_alpha=64,\n",
        "    # Drop out ratio for the layers in LoRA adaptors A and B.\n",
        "    lora_dropout=0.1,\n",
        "    # We fine-tune all linear layers in the model. It might sound a bit large, but the trainable adapter size is still only **1.16%** of the whole model.\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    # Bias parameters to train. 'none' is recommended to keep the original model performing equally when turning off the adapter.\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, peft_config)\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qv57pni8PEd"
      },
      "source": [
        "**That's it!!!** PEFT has made the LoRA setup super easy.\n",
        "\n",
        "An additional bonus is that the PEFT model exposes the same interfaces as a Transformers model. This means that everything from here on is quite similar to the standard model training process using Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prOmbxMV8PEd"
      },
      "source": [
        "## 5. Kick-off a Training Job\n",
        "\n",
        "Similar to conventional Transformers training, we'll first set up a Trainer object to organize the training iterations. There are numerous hyperparameters to configure, but MLflow will manage them on your behalf.\n",
        "\n",
        "To enable MLflow logging, you can specify `report_to=\"mlflow\"` and name your training trial with the `run_name` parameter. This action initiates an [MLflow run](https://mlflow.org/docs/latest/tracking.html#runs) that automatically logs training metrics, hyperparameters, configurations, and the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_Im4KFKOG_A",
        "outputId": "c72e272e-5154-4f98-b6eb-02da4267bc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: codebleu in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: tree-sitter<0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from codebleu) (0.22.3)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.10/dist-packages (from codebleu) (71.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install codebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyrYrZRmOHWx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from codebleu import calc_codebleu\n",
        "import numpy as np\n",
        "\n",
        "# Define custom compute_metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    max_vocab_size = tokenizer.vocab_size\n",
        "\n",
        "    # Ensure logits are tensors before using torch.argmax\n",
        "    if not isinstance(logits, torch.Tensor):\n",
        "        logits = torch.tensor(logits, dtype=torch.float32)  # Adjust dtype as needed\n",
        "\n",
        "    # Convert logits to token IDs using argmax\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Convert tensors to lists of integers for both predictions and labels\n",
        "    predictions = predictions.tolist()\n",
        "    labels = labels.tolist() if isinstance(labels, torch.Tensor) else labels\n",
        "\n",
        "    # Max vocab size and EOS token from tokenizer\n",
        "    eos_token_id = tokenizer.eos_token_id  # End of sentence token\n",
        "\n",
        "    # Handle tokens exceeding max_vocab_size or less than 1\n",
        "    valid_tokens = set(range(max_vocab_size))\n",
        "\n",
        "    # Replace tokens not in the tokenizer's vocab with 1\n",
        "    for i, seq in enumerate(predictions):\n",
        "        valid_seq = []\n",
        "        for j, token in enumerate(seq):\n",
        "            token = int(token)  # Ensure token is cast to int\n",
        "            if token not in valid_tokens:\n",
        "                token = 1  # Replace with a compatible token, like 1\n",
        "            valid_seq.append(token)\n",
        "        predictions[i] = valid_seq\n",
        "\n",
        "    # Filter out -100 values from labels and predictions\n",
        "    filtered_predictions = [[token for token in pred if token != -100] for pred in predictions]\n",
        "    filtered_labels = [[token for token in label if token != -100] for label in labels]\n",
        "\n",
        "    # Decode the predictions and labels\n",
        "    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in filtered_predictions]\n",
        "    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in filtered_labels]\n",
        "\n",
        "    # Compute CodeBLEU score\n",
        "    codebleu_score = calc_codebleu(decoded_labels, decoded_preds, lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25), tokenizer=tokenizer)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = np.mean([pred == label for pred, label in zip(predictions, labels)])\n",
        "\n",
        "    # Calculate evaluation loss\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    labels_tensor = torch.tensor(labels)\n",
        "    eval_loss = loss_fct(logits.view(-1, logits.size(-1)), labels_tensor.view(-1))\n",
        "\n",
        "    return {\n",
        "        \"eval_codebleu\": codebleu_score[\"codebleu\"],\n",
        "        \"eval_accuracy\": accuracy,\n",
        "        \"eval_loss\": eval_loss.item()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet\n",
        "!pip install tree-sitter\n",
        "!pip install -q -U trl"
      ],
      "metadata": {
        "id": "xbWQPieZ3U7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install comet-llm datasets --quiet"
      ],
      "metadata": {
        "id": "HFR5kgEYleCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import comet_llm, os\n",
        "comet_api = \"HvDBna6FPF1cTRHAxDaqXvLce\"\n",
        "os.environ[\"COMET_API_KEY\"] = comet_api"
      ],
      "metadata": {
        "id": "Qs4T3R-IlcBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COMET_LOG_ASSETS\"] = \"true\""
      ],
      "metadata": {
        "id": "nd6AQTBHmTMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = ['prompt with tempalte','summarization']\n",
        "metadata = {\n",
        "    \"model\": \"Mistrail-ai-7b\",\n",
        "    \"max_tokens\": 256\n",
        "}\n",
        "\n",
        "comet_llm.log_prompt(\n",
        "    prompt = prompt,\n",
        "    prompt_template=PROMPT_TEMPLATE,\n",
        "    prompt_template_variables=[sample[\"input\"], sample[\"instruction\"]],\n",
        "    output= response[0][\"generated_text\"],\n",
        "    tags=tags,\n",
        "    duration=duration*1000,\n",
        "    metadata=metadata\n",
        ")"
      ],
      "metadata": {
        "id": "a49kTuCnnQUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.integrations import CometCallback\n",
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "cvPcUPfyom1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade comet-ml>=3.43.2"
      ],
      "metadata": {
        "id": "-I1Y7ywBowFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mmd1XSVd8PEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15659517-be7b-4091-eb0a-d036dda30d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "You are adding a <class 'transformers.integrations.integration_utils.CometCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
            ":DefaultFlowCallback\n",
            "CometCallback\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import os\n",
        "import comet_ml\n",
        "from trl import SFTTrainer\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LinearLR\n",
        "from transformers import EarlyStoppingCallback\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set up the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        max_steps=1000,\n",
        "        save_steps=100,  # Save model every 100 steps\n",
        "        logging_steps=50,  # Log every 50 steps\n",
        "        output_dir=\"output_dir\",\n",
        "        gradient_checkpointing=True,\n",
        "        eval_strategy=\"epoch\",  # Evaluate at the end of every epoch\n",
        "        save_strategy=\"epoch\",  # Save model at the end of every epoch\n",
        "        #load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_codebleu\",  # Metric for selecting the best model\n",
        "        report_to=[\"comet_ml\"],  # Report to Comet.ml for logging\n",
        "        run_name=f\"Mistral-7B-SQL-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n",
        "        warmup_steps=5,\n",
        "        ddp_find_unused_parameters=False,\n",
        "        evaluation_strategy=\"steps\",  # Evaluation can also be done every few steps if needed\n",
        "        eval_steps=100,  # Optionally evaluate every 100 steps as well\n",
        "        save_total_limit=2,  # Limit the number of saved checkpoints\n",
        "        fp16=True,  # Enable mixed precision training for efficiency (if supported by hardware)\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[CometCallback()],\n",
        ")\n",
        "\n",
        "peft_model.config.use_cache = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4355QII88PEe"
      },
      "source": [
        "The training duration may span several hours, contingent upon your hardware specifications. Nonetheless, the primary objective of this tutorial is to acquaint you with the process of fine-tuning using PEFT and MLflow, rather than to cultivate a highly performant SQL generator. If you don't care much about the model performance, you may specify a smaller number of steps or interrupt the following cell to proceed with the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufrEVfoO8PEe"
      },
      "outputs": [],
      "source": [
        "experiment.end()\n",
        "experiment = comet_ml.Experiment(\n",
        "    project_name=\"Mistral-finetune\"\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tree-sitter\n",
        "!pip install tree-sitter-python"
      ],
      "metadata": {
        "id": "ztbm35yDIpSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTusz56Xu10g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from comet_ml import Experiment\n",
        "\n",
        "comet_ml.get_global_experiment().log_model(\"Mistral\", \"./output_dir/checkpoint-500\")"
      ],
      "metadata": {
        "id": "SPTVKKEdwqpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from comet_ml import Experiment\n",
        "exp = Experiment()\n",
        "exp.log_model(\"Mistral\", \"./output_dir/checkpoint-500\")"
      ],
      "metadata": {
        "id": "BoTyHqkDw6NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from comet_ml import API\n",
        "api=API()\n",
        "experiment=api.get(\"eliasammari/mistral-finetune/striped_dragon_2485\")\n",
        "experiment.register_model(\"mistral\")"
      ],
      "metadata": {
        "id": "rIFwVAQww92i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = api.get_model(\"eliasammari\", \"mistral\")\n",
        "md = model.download(\"1.0.0\")"
      ],
      "metadata": {
        "id": "4205b_XfQuBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the name of the model you will be pushing to Hugging Face Model Hub\n",
        "new_model = \"mistral-CodePython_V2\"  # Replace with your desired model name on the Hub\n",
        "\n",
        "# Save the fine-tuned model using the trainer\n",
        "trainer.model.save_pretrained(new_model)\n",
        "\n",
        "# Load the base model and apply the PEFT model to it\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Replace with the ID of the base model used for fine-tuning\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "\n",
        "# Merge the PEFT model into the base model\n",
        "merged_model = PeftModel.from_pretrained(base_model, new_model)\n",
        "merged_model = merged_model.merge_and_unload()\n",
        "\n",
        "# Save the merged model and tokenizer\n",
        "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Push the model and tokenizer to the Hugging Face Model Hub\n",
        "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)\n",
        "\n",
        "print(f\"Model pushed to Hugging Face Hub: https://huggingface.co/{new_model}\")\n"
      ],
      "metadata": {
        "id": "-Wlqt9Q9Cg6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "model = \"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "pipeline = transformers.pipeline(model=model, tokenizer=tokenizer, task=\"text-generation\")\n",
        "\n",
        "sample = test_dataset[100]\n",
        "prompt = PROMPT_TEMPLATE.format(\n",
        "    context=sample[\"input\"], question=sample[\"instruction\"], output=\"\"\n",
        ")  # Leave the answer part blank\n",
        "import time\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    response = pipeline(prompt, max_new_tokens=256, repetition_penalty=1.15, return_full_text=False)\n",
        "duration = time.time() - start_time\n",
        "display_table({\"prompt\": prompt, \"generated_query\": response[0][\"generated_text\"]})"
      ],
      "metadata": {
        "id": "oSa-tddr72gb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "PEFT Llama Finetuning",
      "widgets": {}
    },
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3fa1ac1d88c49b08e70a3ad8dbfb5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_212b04d67a594e57a1f306be844e3609",
              "IPY_MODEL_e6ef19b1491e430cac59c60a6950291a",
              "IPY_MODEL_9fa9e8d898234985944bfafa1238649a"
            ],
            "layout": "IPY_MODEL_052282dac55a49ada5e9d18800832e53"
          }
        },
        "212b04d67a594e57a1f306be844e3609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cf90cfeba154640b067776331e5f140",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8c8fb336b3ab4617af6d3f495ff298db",
            "value": "Map:â€‡100%"
          }
        },
        "e6ef19b1491e430cac59c60a6950291a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fc6ce2a3df3442eaf80bc9e8d8308af",
            "max": 2400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_485245d2597346c3a1204c57e631b8a7",
            "value": 2400
          }
        },
        "9fa9e8d898234985944bfafa1238649a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a62d660dc674acca787a01b17df23a2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fb5908ebae2b4627869309ed04570dc3",
            "value": "â€‡2400/2400â€‡[00:00&lt;00:00,â€‡7283.24â€‡examples/s]"
          }
        },
        "052282dac55a49ada5e9d18800832e53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf90cfeba154640b067776331e5f140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c8fb336b3ab4617af6d3f495ff298db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fc6ce2a3df3442eaf80bc9e8d8308af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485245d2597346c3a1204c57e631b8a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a62d660dc674acca787a01b17df23a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5908ebae2b4627869309ed04570dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5a7eb5f19dc4d1683a81ab57d430072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d795b0768d84498871efd4f00d34119",
              "IPY_MODEL_7f9e3c7189814ee89efdd1e66fa3db45",
              "IPY_MODEL_925f242f341f42d78a45fb360697d9fd"
            ],
            "layout": "IPY_MODEL_9d79ef4e61c6497e82dc2ce0b6857317"
          }
        },
        "7d795b0768d84498871efd4f00d34119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ff5118a5adb49f681254652b125e03d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0553bfc406d74b8aae4c1b32e0947f2c",
            "value": "Map:â€‡100%"
          }
        },
        "7f9e3c7189814ee89efdd1e66fa3db45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0005993190ff4082b8268bd15eb82e29",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa21bf3805b1409ea449dd89cc716f94",
            "value": 600
          }
        },
        "925f242f341f42d78a45fb360697d9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d9f162336f34cc5b0a931d27f60ba74",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_32df9fe475f64df4a6e4c03ef40e7009",
            "value": "â€‡600/600â€‡[00:00&lt;00:00,â€‡6915.21â€‡examples/s]"
          }
        },
        "9d79ef4e61c6497e82dc2ce0b6857317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ff5118a5adb49f681254652b125e03d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0553bfc406d74b8aae4c1b32e0947f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0005993190ff4082b8268bd15eb82e29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa21bf3805b1409ea449dd89cc716f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d9f162336f34cc5b0a931d27f60ba74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32df9fe475f64df4a6e4c03ef40e7009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8dd964c9b4e492e85bdff50d7d0273e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9e4e467a9714caba0362e8d67c93296",
              "IPY_MODEL_cc86c765403f431398067565fbcb93bd",
              "IPY_MODEL_e9a8d66b30a7410bb032a3e6acf792e3"
            ],
            "layout": "IPY_MODEL_1c4f2983271a4ef4982f74c9e9d5363c"
          }
        },
        "d9e4e467a9714caba0362e8d67c93296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b808e536a3d44b4c900cb991c6955b37",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ca0fe1d52f1342e7a396d7daad363844",
            "value": "Map:â€‡100%"
          }
        },
        "cc86c765403f431398067565fbcb93bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbbd78869e274b689341cc583d619c6a",
            "max": 2400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f15bd98459a44c9b5aa5c74acf52fc7",
            "value": 2400
          }
        },
        "e9a8d66b30a7410bb032a3e6acf792e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db1e6bff06374de18be8107fd8baa0d2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2f7e06fb9fbc4601b58bf44c689547e8",
            "value": "â€‡2400/2400â€‡[00:01&lt;00:00,â€‡1446.75â€‡examples/s]"
          }
        },
        "1c4f2983271a4ef4982f74c9e9d5363c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b808e536a3d44b4c900cb991c6955b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca0fe1d52f1342e7a396d7daad363844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbbd78869e274b689341cc583d619c6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f15bd98459a44c9b5aa5c74acf52fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db1e6bff06374de18be8107fd8baa0d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f7e06fb9fbc4601b58bf44c689547e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0c083b87f0345f5b7cec48de96f5035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_853c495485204468bbc503309a19cbea",
              "IPY_MODEL_75f00abd4c6047a49525f5ad6657d000",
              "IPY_MODEL_fa2985ad6b9b40b09a795e849556f9eb"
            ],
            "layout": "IPY_MODEL_b96c8e65c4ad49e891f1a55df774d91c"
          }
        },
        "853c495485204468bbc503309a19cbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac72c70bf01c41a98ddbbd7c235a1fac",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_42a400750a8d4e74ba05af12946e5d9f",
            "value": "Map:â€‡100%"
          }
        },
        "75f00abd4c6047a49525f5ad6657d000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec3de817949b4f9b99c7b2130579d835",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fb73745aa6c4f7dbc5a30a297f8de59",
            "value": 600
          }
        },
        "fa2985ad6b9b40b09a795e849556f9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4018070067345fea6409001c36fe650",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6a540ee4488a47c999fb2cda73910d52",
            "value": "â€‡600/600â€‡[00:00&lt;00:00,â€‡1698.40â€‡examples/s]"
          }
        },
        "b96c8e65c4ad49e891f1a55df774d91c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac72c70bf01c41a98ddbbd7c235a1fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a400750a8d4e74ba05af12946e5d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec3de817949b4f9b99c7b2130579d835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fb73745aa6c4f7dbc5a30a297f8de59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4018070067345fea6409001c36fe650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a540ee4488a47c999fb2cda73910d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01acf93846664a809b3c80514cb9e0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a60647a95d8b4cc695c1211ae2639337",
              "IPY_MODEL_fd78b4af616e45f4b98b98ad6ba12cb1",
              "IPY_MODEL_c43affa33aae471fa536580e7af29e3a"
            ],
            "layout": "IPY_MODEL_ac34ca7b530a4865b11e5ad45861c114"
          }
        },
        "a60647a95d8b4cc695c1211ae2639337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d39d9f016a00479992f256b3f71683a2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_afa592ab430240ddb725f59dc66e8fea",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "fd78b4af616e45f4b98b98ad6ba12cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec1d837c1c4e4cff8183bdf2f6195083",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da1eb0a8ee104e17807ec46c0eea6e3e",
            "value": 2
          }
        },
        "c43affa33aae471fa536580e7af29e3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f6a44f39ac644f6b860047b0893ed5f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cd74d1099ad74691bb4f0b080cf43a4b",
            "value": "â€‡2/2â€‡[00:10&lt;00:00,â€‡â€‡4.83s/it]"
          }
        },
        "ac34ca7b530a4865b11e5ad45861c114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d39d9f016a00479992f256b3f71683a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa592ab430240ddb725f59dc66e8fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec1d837c1c4e4cff8183bdf2f6195083": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da1eb0a8ee104e17807ec46c0eea6e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f6a44f39ac644f6b860047b0893ed5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd74d1099ad74691bb4f0b080cf43a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}