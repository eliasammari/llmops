{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "RDz_7jlIAhoR"
      },
      "outputs": [],
      "source": [
        "!pip install langchain  transformers  langchain-community langchain-core langchainhub langchain_experimental langsmith langchain_google_genai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"DST-LLMops-Elias-LLM-01\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAauQdTvqz0ynb15ZBL6lrhwzmpz5kkEh8\""
      ],
      "metadata": {
        "id": "p_r66uwDsqUu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r4NCzPU8rJ43",
        "outputId": "26c2482b-9ce6-480b-f9b8-e08b58cd9cf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langsmith import Client\n",
        "from langchain import hub\n",
        "import pandas as pd\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "from langchain.agents import Tool\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "dataset = pd.read_csv(filepath)\n",
        "\n",
        "dataset.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn4b3i1l9ckA"
      },
      "source": [
        "## un LLM avec un modèle HF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_core langchain_experimental langchain_google_genai --quiet\n",
        "!pip install transformers peft accelerate bitsandbytes datasets -q -U\n",
        "!pip install --upgrade comet-ml>=3.43.2 -qU\n",
        "!pip install -U bitsandbytes -qU"
      ],
      "metadata": {
        "id": "_k-PSqE1ha5D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_react_agent, initialize_agent, AgentType\n",
        "from langchain.agents.format_scratchpad import format_log_to_str\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.tools.render import render_text_description\n",
        "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
        "from langchain.agents.format_scratchpad import format_log_to_str\n",
        "from langchain import hub\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import datetime as dt\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from peft import PeftModel\n",
        "\n",
        "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "dataset = pd.read_csv(filepath)\n",
        "from comet_ml import API\n",
        "api=API(api_key=\"HvDBna6FPF1cTRHAxDaqXvLce\")\n",
        "experiment=api.get(\"eliasammari/mistral-finetune/striped_dragon_2485\")\n",
        "model = api.get_model(\"eliasammari\", \"mistral\")\n",
        "md = model.download(\"1.0.0\")\n",
        "#\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "# Initialize Google Generative AI Model with an API key\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    # Load the model with 4-bit quantization\n",
        "    load_in_4bit=True,\n",
        "    # Use double quantization\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    # Use 4-bit Normal Float for storing the base model weights in GPU memory\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # De-quantize the weights to 16-bit (Brain) float before the forward/backward pass\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ],
      "metadata": {
        "id": "a70ICdA7hpux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cb7f1b-b753-4a74-86fd-45e3a4f8851b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Remote Model 'eliasammari/mistral:1.0.0' download has been started asynchronously.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still downloading 36 file(s), remaining 4.32 GB/4.32 GB\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/README.md' has been overwritten by asset '4821493920f5406b812ae3426aaeda88' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/adapter_model.safetensors' has been overwritten by asset '55490397b45e43b282e3f85391919b69' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/rng_state.pth' has been overwritten by asset '697de578a2714943a904bff18a112088' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/tokenizer_config.json' has been overwritten by asset '7eac113b7554460ca68a73134c7a4575' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/scheduler.pt' has been overwritten by asset '82349fae07b54a3c95fa5716264ae3da' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/trainer_state.json' has been overwritten by asset '8c2602cb10e347229cbcb30648bfcce2' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/trainer_state.json' has been overwritten by asset '943a5a90b45d46adbe1da544f07ccc2f' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/tokenizer.json' has been overwritten by asset '952ebfec0126486bb938f13a0bd94d21' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/adapter_model.safetensors' has been overwritten by asset '9bd9281a87264e80b7fe5594347015c9' of remote model\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still downloading 19 file(s), remaining 3.40 GB/4.32 GB, Throughput 62.76 MB/s, ETA ~56s\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still downloading 19 file(s), remaining 2.42 GB/4.32 GB, Throughput 67.06 MB/s, ETA ~37s\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/special_tokens_map.json' has been overwritten by asset '9cabff691f7646a7ba8c25b6917f0cbd' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/training_args.bin' has been overwritten by asset '9fea0a818f8e425daacf9e48d1f68e1f' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/scheduler.pt' has been overwritten by asset 'adef94bc69634106805317978de66063' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/adapter_config.json' has been overwritten by asset 'af68cb9bb33443b2a4af10393889b151' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/tokenizer.model' has been overwritten by asset 'b43fc37ee54c4bf184f65e5e18ea648c' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/README.md' has been overwritten by asset 'b95997558c944ac4b4f6dd74d2a4c1ec' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/optimizer.pt' has been overwritten by asset 'c8c7ed0b41c4402eb2dec82e271173e0' of remote model\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still downloading 12 file(s), remaining 1.45 GB/4.32 GB, Throughput 66.11 MB/s, ETA ~23s\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/tokenizer_config.json' has been overwritten by asset 'c914257f19074c25b4a999341ff9a483' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/optimizer.pt' has been overwritten by asset 'd4a0b30d82af49f5a15d4cf893bf8e54' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/tokenizer.model' has been overwritten by asset 'd6dee6f3fca64e709daff42f8ffd9286' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/adapter_config.json' has been overwritten by asset 'de2f3ce966034c349cadea6cb74f7db5' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/special_tokens_map.json' has been overwritten by asset 'de898b70cf3d462e917b4618fd1f331a' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/rng_state.pth' has been overwritten by asset 'f28eda03212440e8a1e9429d9a58de7d' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/tokenizer.json' has been overwritten by asset 'f40d854f0d954d54a6aa4e1200652406' of remote model\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m File '/tmp/tmpy4rv_gjh/checkpoint-500/training_args.bin' has been overwritten by asset 'f42147616e3e45c9a2c65889e194e02b' of remote model\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still downloading 2 file(s), remaining 761.35 MB/4.32 GB, Throughput 48.19 MB/s, ETA ~16s\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still downloading 2 file(s), remaining 245.35 MB/4.32 GB, Throughput 34.36 MB/s, ETA ~8s\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Remote Model 'eliasammari/mistral:1.0.0' has been successfully downloaded.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Downloaded asset files is in '/tmp/tmpy4rv_gjh' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes -qU\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "lzdRRL8bh1Je",
        "outputId": "183405cb-450c-4144-81c3-785d43062cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "604bc32dff0741cba5bb0cfba6391533",
            "318452d8f85e4f68b3c1da584727fb04",
            "b0b20fb5ffb44d818098d657ee6132fd",
            "22ad28ac576d4dc7aef814441b1322cc",
            "9bd8c11d62b040588fd28a0fbb9b68d1",
            "1ffb93b517f5453f8d3a484a2ce7ab60",
            "5c8b0a6fc6a54e5fa629e79d54e53ed6",
            "aed44e3366074f33815f578feaaf0d10",
            "3df894598fec451b9078652ce7ef2608",
            "e8fa98cef39d4f9da2edb2be75a56fa8",
            "7f0eaf92243443a98b2925a621534f2d"
          ]
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "604bc32dff0741cba5bb0cfba6391533"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base_model, \"/tmp/tmpy4rv_gjh/checkpoint-500/\", adapter_config=\"adapter_config.json\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
      ],
      "metadata": {
        "id": "KSQb9MJb4X-f"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client=Client()\n",
        "#session = client.create_project(\"final-eval\", description=\"Projet DST LLMops\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"final-eval\""
      ],
      "metadata": {
        "id": "_CEs5KqV_mA4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OzfsCO-39WM5"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]='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'\n",
        "os.environ[\"HF_TOKEN\"]='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'\n",
        "\n",
        "checkpoint1 = \"gemini-pro\"\n",
        "checkpoint2 = 'ibm-granite/granite-3b-code-instruct'\n",
        "checkpoint3 = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "checkpoint4 = \"google/gemma-2b-it\"\n",
        "checkpoint5 = \"mistralai/Mistral-7B-v0.1\"\n",
        "checkpoint7 = \"azurehorizon/gemma-2b-it-Code-Instruct-ft-122k_alpaca_style\"\n",
        "\n",
        "\n",
        "gllm = GoogleGenerativeAI(model=\"gemini-pro\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint2)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint2,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")"
      ],
      "metadata": {
        "id": "Skl1bfPZrU6q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text-generation pipeline\n",
        "pip = pipeline('text-generation', model=model, tokenizer=tokenizer,  max_new_tokens=150, max_length=1024)\n",
        "\n",
        "# Wrap the pipeline in a HuggingFacePipeline LLM\n",
        "llm = HuggingFacePipeline(pipeline=pip)"
      ],
      "metadata": {
        "id": "AOme1lT8spdG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prompt"
      ],
      "metadata": {
        "id": "SZjiPWXkPAA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Kc-kGzoUyHAs"
      },
      "outputs": [],
      "source": [
        "def get_completion(query: str, model, tokenizer) -> str:\n",
        "  prompt = \"\"\"You are a python developer with good experience .\n",
        "  A user has asked : {question}.\n",
        "  The data looks like this :\n",
        "  {data}.\n",
        "  These are all the columns :\n",
        "  {columns}.\n",
        "\n",
        "  Do not add any other columns other than these.\n",
        "\n",
        "  Rules :\n",
        "\n",
        "  1. Always use matplot lib,seaborn and plotly to plot the charts for the end user.\n",
        "  2. Generate the python code to solve the problem. Also provide your assumptions and  any other details you want to share.\n",
        "  3. Do Not Generate columns on your own.\n",
        "  5. Always start with ```python and end with plt.show()```\n",
        "\n",
        "  Answer :\"\"\"\n",
        "\n",
        "  question = query\n",
        "  data = \"\"\"filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "              dataset = pd.read_csv(filepath)\n",
        "              Index(['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure'], dtype='object').\"\"\"\n",
        "  columns = dataset.columns\n",
        "\n",
        "  prompt = prompt.format(question=question,data=data, columns=columns)\n",
        "\n",
        "  prompt = prompt.format(query=query)\n",
        "\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    # decoded = tokenizer.batch_decode(generated_ids)\n",
        "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "  return decoded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(\"Plot a time serie of humidity in python using seaborn\")"
      ],
      "metadata": {
        "id": "SODaQFn_JMrN",
        "outputId": "e30394bf-d799-4f5b-8b20-5c27ca5b306c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=150) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Plot a time serie of humidity in python using seabornHere are the inputs Not applicable \\nmodelimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Generate a random time series of humidity\\nhumidity = sns.time_series(data=100, start=0, end=100, n=1000)\\n\\n# Plot the time series\\nsns.lineplot(x=humidity.index, y=humidity.values)\\nplt.xlabel('Time')\\nplt.ylabel('Humidity')\\nplt.title('Humidity Time Series')\\nplt.show() \\nmodelThe code above generates a random time series of humidity and plots it using Seaborn. Here are the inputs Not applicable \\nHere are the outputs\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8c5rMVLfzgQ4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cBu1xmTAGwe"
      },
      "source": [
        "## un LLM chain avec prompt\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a python developer with good experience .\n",
        "  A user has asked : {question}.\n",
        "  The data looks like this :\n",
        "  {data}.\n",
        "  These are all the columns :\n",
        "  {columns}.\n",
        "\n",
        "  Do not add any other columns other than these.\n",
        "\n",
        "  Rules :\n",
        "\n",
        "  1. Always use matplot lib,seaborn and plotly to plot the charts for the end user.\n",
        "  2. Generate the python code to solve the problem. Also provide your assumptions and  any other details you want to share.\n",
        "  3. Do Not Generate columns on your own.\n",
        "  5. Always start with ```python and end with plt.show()``` \"\"\"\n",
        "\n",
        "question = \"Affiche un grpah sous forme de ligne pour montrer l'evolution de l'humidité sur l'année 2013\"\n",
        "data = \"\"\"filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "              dataset = pd.read_csv(filepath)\n",
        "              Index(['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure'], dtype='object').\"\"\"\n",
        "columns = dataset.columns\n",
        "\n",
        "prompt = prompt.format(question=question,data=data, columns=columns)"
      ],
      "metadata": {
        "id": "kwdeEHgWAoMY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YtLxm2tMAWvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e531a828-31c7-4e99-943f-498d3f424699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "gllm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "prompt_template = \"\"\"\"\n",
        "generate a python code using plotly {query}\n",
        "\n",
        "use this dataframe :\n",
        "    filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "    dataset = pd.read_csv(filepath)\n",
        "    columns: ['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
        "    types: [date, float, float, float, float]\n",
        "\n",
        "must import libraries\n",
        "must not define new variables\n",
        "must not use def function\n",
        "must start with '''python\n",
        "must end with '''\n",
        "\n",
        "you have to respect the output format.\n",
        "\"\"\"\n",
        "\n",
        "query = \"generate a python code using plotly to average humidity per month in year 2013\"\n",
        "prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=['query']\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=gllm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain.run(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDkRoEaN_yYZ",
        "outputId": "19bcfdcd-90df-44ec-c45e-a1927ef428a8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'''python\n",
            "import pandas as pd\n",
            "import plotly.express as px\n",
            "\n",
            "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
            "dataset = pd.read_csv(filepath)\n",
            "\n",
            "monthly_humidity = dataset[dataset['date'].dt.year == 2013].groupby(\n",
            "    [dataset['date'].dt.month]).mean()\n",
            "\n",
            "fig = px.line(monthly_humidity, x=monthly_humidity.index, y=\"humidity\")\n",
            "fig.show()\n",
            "'''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Load the dataset\n",
        "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "dataset = pd.read_csv(filepath)\n",
        "\n",
        "# Convert 'date' to datetime format\n",
        "dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "\n",
        "# Filter data for the year 2013\n",
        "dataset_2013 = dataset[dataset['date'].dt.year == 2013]\n",
        "\n",
        "# Calculate average humidity per month\n",
        "monthly_avg_humidity = dataset_2013.groupby(dataset_2013['date'].dt.month)['humidity'].mean().reset_index()\n",
        "monthly_avg_humidity.columns = ['Month', 'Average Humidity']\n",
        "\n",
        "# Plot the average humidity per month for 2013\n",
        "fig = px.bar(monthly_avg_humidity, x='Month', y='Average Humidity', title='Average Humidity Per Month in 2013')\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "j3KyYdjupsOp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "c05b5b06-c802-421a-9210-7362a9c01f90"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"fe568771-d86d-4cf3-9abe-63c9587ded60\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"fe568771-d86d-4cf3-9abe-63c9587ded60\")) {                    Plotly.newPlot(                        \"fe568771-d86d-4cf3-9abe-63c9587ded60\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Month=%{x}\\u003cbr\\u003eAverage Humidity=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12],\"xaxis\":\"x\",\"y\":[73.02880184331798,71.93856292517007,57.41724270353303,34.61210317460317,28.938248847926268,58.75813492063492,74.87338709677418,76.7571044546851,64.5527380952381,70.25560675883257,66.24595238095237,79.13440860215053],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Month\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Average Humidity\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Average Humidity Per Month in 2013\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fe568771-d86d-4cf3-9abe-63c9587ded60');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD_j70p2-jOH"
      },
      "source": [
        "##Un agent React\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "slCExGkf-m3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "ff7024c5-bd54-4199-c723-71d00ebeff62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning:\n",
            "\n",
            "The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning:\n",
            "\n",
            "Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "\n",
            "Exception in thread Thread-16 (generate):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1874, in generate\n",
            "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1266, in _validate_generated_length\n",
            "    raise ValueError(\n",
            "ValueError: Input length of input_ids is 380, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-610fbedbc511>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#Prédiction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0magent_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             outputs = (\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1606\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m   1609\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001b[1;32m   1313\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1314\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1315\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m                 for a in self._iter_next_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:\n\u001b[1;32m   1313\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1314\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1315\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m                 for a in self._iter_next_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m             \u001b[0;31m# Call the LLM to see what to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m             output = self.agent.plan(\n\u001b[0m\u001b[1;32m   1343\u001b[0m                 \u001b[0mintermediate_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36mplan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Because the response from the plan is not a generator, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;31m# accumulate the output into final output and return that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunnable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfinal_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3260\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3261\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 3262\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m     async def atransform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3248\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 3249\u001b[0;31m         yield from self._transform_stream_with_config(\n\u001b[0m\u001b[1;32m   3250\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   2052\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2054\u001b[0;31m                     \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2055\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfinal_output_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3209\u001b[0m                 \u001b[0mfinal_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3211\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3212\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mgot_first_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0michunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0;31m# The default implementation of transform is to buffer input and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;31m# then call stream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5299\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5300\u001b[0m     ) -> Iterator[Output]:\n\u001b[0;32m-> 5301\u001b[0;31m         yield from self.bound.transform(\n\u001b[0m\u001b[1;32m   5302\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5303\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgot_first_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     async def atransform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                     ),\n\u001b[1;32m    570\u001b[0m                 )\n\u001b[0;32m--> 571\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0mgeneration\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGenerationChunk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                 for chunk in self._stream(\n\u001b[0m\u001b[1;32m    556\u001b[0m                     \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 ):\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerationChunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/streamers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_signal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Les outils\n",
        "pythonttol = PythonREPLTool()\n",
        "\n",
        "tools = [Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "    func=pythonttol.run,\n",
        ")]\n",
        "\n",
        "#Définition du prompt template\n",
        "instructions = \"\"\"You are an agent designed to execute python code to answer questions.\n",
        "\"\"\"\n",
        "base_prompt = hub.pull(\"khodak/react-agent-template\")\n",
        "prompt = base_prompt.partial(instructions=instructions)\n",
        "\n",
        "#print(prompt.template)\n",
        "\n",
        "\n",
        "agent = create_react_agent(llm,tools, prompt = prompt)\n",
        "agent_executor = AgentExecutor(input=data, agent=agent, tools=tools, verbose=True,handle_parsing_errors=True)\n",
        "\n",
        "\n",
        "#Prédiction\n",
        "\n",
        "agent_executor.invoke({\"input\":query})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5lyj0l5Fswu"
      },
      "source": [
        "## une chaine convernationel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdU0dPWyG4W_"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain  --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRz2u4_i3D4M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent, initialize_agent, AgentType\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.tools.render import render_text_description\n",
        "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
        "from langchain.agents.format_scratchpad import format_log_to_str\n",
        "# Initialize Python REPL tool\n",
        "python_repl = PythonREPL()\n",
        "\n",
        "repl_tool = Tool(\n",
        "    name=\"python_repl\",\n",
        "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\",\n",
        "    func=python_repl.run,\n",
        ")\n",
        "\n",
        "tools = [repl_tool]\n",
        "\n",
        "# Define the prompt template with history\n",
        "instructions = \"\"\"\n",
        "You are an agent designed to answer questions and execute Python code when needed.\n",
        "You have access to the following tools: {tools}.\n",
        "\n",
        "The data looks like this:\n",
        "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "dataset = pd.read_csv(filepath)\n",
        "Index(['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure'], dtype='object').\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Get the base prompt and modify it with instructions\n",
        "base_prompt = hub.pull(\"khodak/react-agent-template\")\n",
        "base_prompt = base_prompt.partial(instructions=instructions)\n",
        "\n",
        "prompt = base_prompt.partial(\n",
        "    tools=render_text_description(tools),\n",
        "    tool_names=\", \".join([t.name for t in tools])\n",
        ")\n",
        "llm_with_stop = gllm.bind(stop=[\"\\nObservation\"])\n",
        "\n",
        "agent = (\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"agent_scratchpad\": lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
        "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "    }\n",
        "    | prompt\n",
        "    | llm_with_stop\n",
        "    | ReActSingleInputOutputParser()\n",
        ")\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, memory=memory)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"can you print the first ten lines ?\"})[\"output\"]\n",
        "\n",
        "agent_executor.invoke({\"input\": \"How many lines did I just asked you to print previously ?\"})[\"output\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        break\n",
        "    response = agent_executor.invoke({\"input\": user_input})\n",
        "    print(f\"Agent: {response['output']}\")"
      ],
      "metadata": {
        "id": "JL2uU6FzP38R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "rw8INU1U6GrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "dyls-pH3t9Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=[\n",
        "\"\"\"\n",
        "generate a python code using plotly Genere un code python qui affiche l'evolution de l'humidité a travers le temps avec matplotlib ?\n",
        "use this dataframe :\n",
        "    filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "    dataset = pd.read_csv(filepath)\n",
        "    columns: ['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
        "    types: [date, float, float, float, float]\n",
        "\n",
        "must import libraries\n",
        "must not define new variables\n",
        "must not use def function\n",
        "must start with '''python\n",
        "must end with '''\n",
        "\n",
        "you have to respect the output format.\n",
        "\n",
        "\"\"\",\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "\"\"\"\n",
        "generate a python code using plotly Genere un code python qui affiche l'evolution de wind_speed color='red' ?\n",
        "\n",
        "use this dataframe :\n",
        "    filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "    dataset = pd.read_csv(filepath)\n",
        "    columns: ['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
        "    types: [date, float, float, float, float]\n",
        "\n",
        "must import libraries\n",
        "must not define new variables\n",
        "must not use def function\n",
        "must start with '''python\n",
        "must end with '''\n",
        "\n",
        "you have to respect the output format.\n",
        "\n",
        "\"\"\",\n",
        "#-----------------------------------------------------------------------\n",
        "\"\"\"\n",
        "generate a python code using plotly to average humidity per month in year 2013 ?\n",
        "\n",
        "use this dataframe :\n",
        "    filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "    dataset = pd.read_csv(filepath)\n",
        "    columns: ['date', 'meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
        "    types: [date, float, float, float, float]\n",
        "\n",
        "must import libraries\n",
        "must not define new variables\n",
        "must not use def function\n",
        "must start with '''python\n",
        "must end with '''\n",
        "\n",
        "you have to respect the output format.\n",
        "\n",
        "\"\"\",\n",
        "]"
      ],
      "metadata": {
        "id": "KmB6koq56oKs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=[\"\"\"\n",
        "'''python\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "dataset = pd.read_csv(filepath)\n",
        "\n",
        "# Convert 'date' to datetime format\n",
        "dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "\n",
        "# Plot the evolution of humidity over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(dataset['date'], dataset['humidity'], label='Humidity', color='b')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Humidity')\n",
        "plt.title('Evolution of Humidity Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "'''\n",
        "\"\"\",\n",
        "#------------------------------------------------------\n",
        "\"\"\"\n",
        "'''python\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Load the dataset\n",
        "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "dataset = pd.read_csv(filepath)\n",
        "\n",
        "# Convert 'date' to datetime format\n",
        "dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "\n",
        "# Plot the evolution of wind_speed over time with color red\n",
        "fig = px.line(dataset, x='date', y='wind_speed', title='Evolution of Wind Speed Over Time', color_discrete_sequence=['red'])\n",
        "fig.update_traces(line=dict(color='red'))\n",
        "fig.show()\n",
        "'''\n",
        "\"\"\",\n",
        "#------------------------------------------------------\n",
        "\"\"\"\n",
        "'''python\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Load the dataset\n",
        "filepath = \"/content/DailyDelhiClimateTrain.csv\"\n",
        "dataset = pd.read_csv(filepath)\n",
        "\n",
        "# Convert 'date' to datetime format\n",
        "dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "\n",
        "# Filter data for the year 2013\n",
        "dataset_2013 = dataset[dataset['date'].dt.year == 2013]\n",
        "\n",
        "# Calculate average humidity per month\n",
        "monthly_avg_humidity = dataset_2013.groupby(dataset_2013['date'].dt.month)['humidity'].mean().reset_index()\n",
        "monthly_avg_humidity.columns = ['Month', 'Average Humidity']\n",
        "\n",
        "# Plot the average humidity per month for 2013\n",
        "fig = px.bar(monthly_avg_humidity, x='Month', y='Average Humidity', title='Average Humidity Per Month in 2013')\n",
        "fig.show()\n",
        "'''\n",
        "\n",
        "\"\"\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "bDTiMuf16oz2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()"
      ],
      "metadata": {
        "id": "mlEAtS-U6qrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = client.create_dataset(\"benchmark-dataset\")\n",
        "client.create_examples(inputs=[{\"question\": x} for x in input],\n",
        "                        outputs= [{\"answer\":y } for y in output],\n",
        "                        dataset_id=dataset.id)"
      ],
      "metadata": {
        "id": "E0TcfQjQ6I5c"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig\n",
        "from langchain.evaluation import EvaluatorType\n",
        "evalConfig= RunEvalConfig(eval_llm=gllm,\n",
        "                          evaluators=[\n",
        "                           RunEvalConfig.Criteria (\"conciseness\") ,\n",
        "                           RunEvalConfig.Criteria (\"relevance\") ,\n",
        "                           RunEvalConfig.Criteria (\"coherence\") ,\n",
        "                           EvaluatorType.STRING_DISTANCE,\n",
        "                           EvaluatorType.QA,\n",
        "                           RunEvalConfig.Criteria ({\"has_matplotlib\": \"Does the text contain the word matplotlib\"}),\n",
        "                           RunEvalConfig.Criteria ({\"has_plot\": \"Does the text contain the word plot\"})\n",
        "\n",
        "                          ])"
      ],
      "metadata": {
        "id": "Sk6Qm-Qa9BNn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "id": "ihv9bOZS-Kl-",
        "outputId": "8d3eff05-9be0-47c8-a0f4-faf0b854c48c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (3.9.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import run_on_dataset\n",
        "result= run_on_dataset(\n",
        "    client= client,\n",
        "    dataset_name=\"benchmark-dataset\",\n",
        "    llm_or_chain_factory=llm,\n",
        "    dataset_id=dataset.id,\n",
        "    evaluation =evalConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "iiwjK63g9kgV"
      },
      "execution_count": 42,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "SZjiPWXkPAA_",
        "iD_j70p2-jOH",
        "n5lyj0l5Fswu"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "604bc32dff0741cba5bb0cfba6391533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_318452d8f85e4f68b3c1da584727fb04",
              "IPY_MODEL_b0b20fb5ffb44d818098d657ee6132fd",
              "IPY_MODEL_22ad28ac576d4dc7aef814441b1322cc"
            ],
            "layout": "IPY_MODEL_9bd8c11d62b040588fd28a0fbb9b68d1"
          }
        },
        "318452d8f85e4f68b3c1da584727fb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ffb93b517f5453f8d3a484a2ce7ab60",
            "placeholder": "​",
            "style": "IPY_MODEL_5c8b0a6fc6a54e5fa629e79d54e53ed6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b0b20fb5ffb44d818098d657ee6132fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aed44e3366074f33815f578feaaf0d10",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3df894598fec451b9078652ce7ef2608",
            "value": 2
          }
        },
        "22ad28ac576d4dc7aef814441b1322cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8fa98cef39d4f9da2edb2be75a56fa8",
            "placeholder": "​",
            "style": "IPY_MODEL_7f0eaf92243443a98b2925a621534f2d",
            "value": " 2/2 [01:29&lt;00:00, 40.92s/it]"
          }
        },
        "9bd8c11d62b040588fd28a0fbb9b68d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffb93b517f5453f8d3a484a2ce7ab60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8b0a6fc6a54e5fa629e79d54e53ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aed44e3366074f33815f578feaaf0d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df894598fec451b9078652ce7ef2608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8fa98cef39d4f9da2edb2be75a56fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0eaf92243443a98b2925a621534f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}